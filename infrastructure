Infrastructure
Hazelcast, Reddis, Ignite (распределенные ин-мемори кэши)

Hazelcast это распределенный кэш, распределенные структуры данных, компьютинг и стриминг, его основная цель ускорение приложения
Архитектура Hazlecast построена на Pool'ах, на входе IO Pool и далее 3 пула зависящих от задач - Generic Pool за вычисления, Partition Pool за работу со структурами ключ-значение, Jet Pool за стриминг
Hazelcast в отличии от Reddis многопоточный, т.е. loop крутится на каждом потоке, данные распределяются по потокам в зависимости от отнесения к какому-то под-сету например на основании хэша ключа, таким образом избегаем конкурентных запросов к одинм и тем же данным из разных потоков но в отличии от Reddis разделение по потокам происходит еще и по типам операций чтобы в одну очередь не попадали одновременно и "короткие" и "длинные" операции, общение между потоками происходит в рамках очередей (SEDA) stage event-drivin architecture, например IO-Pul отвечающий за прием, отправку и парсинг сообщений, общается с Partition Pool, который отвечает за обработку, через очередь

Reddis - брокер сообщений, СУБД, inmemory cache -  условно однопоточный, все запросы приходят в один поток в котором крутится бесконечный event-loop обрабатывающий события 
данные распределяются по потокам в зависимости от отнесения к какому-то под-сету например на основании хэша ключа, таким образом избегаем конкурентных запросов к одинм и тем же данным из разных потоков 
для исполнения разных по длительности операций рекомендуется развернуть отдельные кластера для каждого типа 
Reddis поддерживает множество типов данных включая String, List, Set, Sorted Set, Hashe, Bitmap, Hyperlog, Geospatial Index

Kafka - распределенный брокер сообщений
архитектура предлагает подход Producer -> Cluster -> Consumer
кластер состоит из множества брокеров
взаимодействие происходит через логическую единицу Topic в брокере
механика доставки сообщений - PULL т.е. consumer приходит в Broker и спрашивает есть ли для него новые данные
каждый Topic состоит из нескольких Partition, запись всегда осуществляется в конец, при этом писать можно в несколько партиций т.е. происходит параллелезация на запись
по итогу в каждой партиции есть сообщения пронумерованные с 0 и выше поскольку партиция может быть очень большой, то она делится на сегменты, запись осуществляется в активный сегмент
один из брокеров является controller - он координирует работу кластера
каждый топик распределяется по брокерам разделяясь по партициям
дальше в зависимости от replication factor партиции реплицируются на брокеры т.е. если количество брокеров и replication factor равны, то каждая партиция будет в каждом брокере, каждый брокер назначается лидером для определенный партиций, лидерство распределается по-возможности равномерно
репликация происходит от брокера-лидера к брокерам-фоловерам если у брокера-фоловера есть реплицируемая партиция, фоловер сам запрашивает данные с лидера
если лидер недоступен выделяется новый лидер, возвращаясь лидер становится фоловером, с определенной периодичностью происходит перебалансировка
Producer отправляет в брокер сообщение в формате ключ-значение, кафка воспринимает ключ как массив байтов, по ключу считается хэш и по нему брокер распределяет данные по партициям, если нет ключа то распределение - по раунд-робину
важно, что в какую партицию попадет партиция решает не брокер а Producer(??? может "в какой топик" ???)
значение это тоже массив байтов
у Producer есть подтверждение записи (ack), уровни которого 0 (без подтверждения), 1 (подтверждение записи в лидера) и all (подтверждение репликации) в минимально необходимое количество реплик
Consumer может читать реплицированные данные из нескольких партиций попеременно, останавливаясь в одной партиции и продолжая в другой
consumer может вызвать у кафки commit offset чтобы запомнить до какой записи он дочитал
можно установить consumer group из нескольких консьюмеров и сделать так, чтобы каждый из них читать данные только с определенных партиций а не со всех
ZooKeeper отвечает за выбор контроллера, конфигурацию топиков, какие реплики на каких нодах находятся и в каком они состоянии, также там хранится cluster state т.е. сколько брокеров онлайн
контроллер решает какой из брокеров будет лидером у какой партиции
